{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "281a75273d4748a1a6687d9d0d2b2c50"
   },
   "source": [
    "# Project Model-Mesh Serving Post-Install Demo Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4520043df94e40c38679956f18f1ad21"
   },
   "source": [
    "## Cluster Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisites \n",
    "- ETCD for meta store\n",
    "- COS/S3 for model storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure you are logged into the cluster with kubectl or oc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create namespace 'wmlserving'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using project \"wmlserving\" on server \"api.anhuong-dev-jun-10.cp.fyre.ibm.com\""
     ]
    }
   ],
   "source": [
    "!oc new-project wmlserving\n",
    "# ensure in wmlserving namespace\n",
    "!oc project wmlserving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Anh.Uong@ibm.com/github.ibm.com/ai-foundation/model-serving\n"
     ]
    }
   ],
   "source": [
    "%cd ../../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use install scripts to install Model-Mesh Serving and dependencies (etcd and minio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: ARTIFACTORY_USER=\"anh.uong@ibm.com\"\n",
      "curl: (22) The requested URL returned error: 401\n"
     ]
    }
   ],
   "source": [
    "# REPLACE ARTIFACTORY_USER and ARTIFACTORY_APIKEY with your credentials\n",
    "!curl -sSLf -o wmlserving-install.sh -u <ARTIFACTORY_USER>:<ARTIFACTORY_APIKEY> https://na.artifactory.swg-devops.com/artifactory/wcp-ai-foundation-team-generic-virtual/model-serving/wml-serving-0.5.0_165-install.sh\n",
    "!chmod +x wmlserving-install.sh\n",
    "!./wmlserving-install.sh -n wmlserving -u <ARTIFACTORY_USER> -a <ARTIFACTORY_APIKEY> --quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                         READY   STATUS    RESTARTS   AGE\n",
      "pod/wmlserving-controller-568c45b959-xfzlk   1/1     Running   0          6m38s\n",
      "\n",
      "NAME                  TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE\n",
      "service/wml-serving   ClusterIP   None         <none>        8033/TCP   21m\n",
      "\n",
      "NAME                                            READY   UP-TO-DATE   AVAILABLE   AGE\n",
      "deployment.apps/wml-serving-mlserver-0.x        0/0     0            0           21m\n",
      "deployment.apps/wml-serving-msp-ml-server-0.x   0/0     0            0           21m\n",
      "deployment.apps/wml-serving-triton-2.x          0/0     0            0           21m\n",
      "deployment.apps/wmlserving-controller           1/1     1            1           22m\n",
      "\n",
      "NAME                                                       DESIRED   CURRENT   READY   AGE\n",
      "replicaset.apps/wml-serving-mlserver-0.x-78fcbcbc6d        0         0         0       21m\n",
      "replicaset.apps/wml-serving-msp-ml-server-0.x-57c69c6bfb   0         0         0       21m\n",
      "replicaset.apps/wml-serving-triton-2.x-6c9784c9db          0         0         0       21m\n",
      "replicaset.apps/wmlserving-controller-568c45b959           1         1         1       6m39s\n",
      "replicaset.apps/wmlserving-controller-6b8db9857f           0         0         0       22m\n"
     ]
    }
   ],
   "source": [
    "# Model-Mesh Serving should be installed\n",
    "!kubectl get all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-req for Triton serving for OCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clusterrole.rbac.authorization.k8s.io/system:openshift:scc:anyuid added: \"wmlserving\"\n"
     ]
    }
   ],
   "source": [
    "!oc adm policy add-scc-to-user anyuid system:serviceaccount:model-serving:wmlserving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "45bee5c451714be6912b060f3c73af39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting grpcio\n",
      "  Downloading grpcio-1.38.0-cp39-cp39-macosx_10_10_x86_64.whl (3.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.9 MB 5.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio-tools\n",
      "  Downloading grpcio_tools-1.38.0-cp39-cp39-macosx_10_10_x86_64.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 35.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/site-packages (1.20.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/site-packages (3.3.4)\n",
      "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.9/site-packages (from grpcio) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from grpcio-tools) (52.0.0)\n",
      "Requirement already satisfied: protobuf<4.0dev,>=3.5.0.post1 in /usr/local/lib/python3.9/site-packages (from grpcio-tools) (3.17.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/site-packages (from matplotlib) (8.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.9/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.9/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/site-packages (from matplotlib) (1.3.1)\n",
      "Installing collected packages: grpcio, grpcio-tools\n",
      "Successfully installed grpcio-1.38.0 grpcio-tools-1.38.0\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install python dependencies\n",
    "!pip3 install grpcio grpcio-tools numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data for MNIST\n",
    "!curl -sSLfo mnist.npz https://s3.amazonaws.com/img-datasets/mnist.npz\n",
    "\n",
    "# Download triton grpc protos\n",
    "!curl -sSfLo model_config.proto \"https://raw.githubusercontent.com/triton-inference-server/server/v2.10.0/src/core/model_config.proto\"\n",
    "!curl -sSfLo grpc_service.proto \"https://raw.githubusercontent.com/triton-inference-server/server/v2.10.0/src/core/grpc_service.proto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. grpc_service.proto model_config.proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import grpc_service_pb2\n",
    "import grpc_service_pb2_grpc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cb7a68822a3408cb5922dd9a5babfbe",
    "tags": []
   },
   "source": [
    "## Deploy Predictor and Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secret Key for storage should be added to secret \"storage-config\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "    name: storage-config\n",
    "stringData:\n",
    "    myStorage: |\n",
    "        {\n",
    "            \"type\": \"s3\",\n",
    "            \"access_key_id\": \"xxx\",\n",
    "            \"secret_access_key\": \"xxx\",\n",
    "            \"endpoint_url\": \"https://s3.us-south.cloud-object-storage.appdomain.cloud\",\n",
    "            \"region\": \"us-south\",\n",
    "            \"default_bucket\": \"\"\n",
    "        }\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret/storage-config patched (no change)\n"
     ]
    }
   ],
   "source": [
    "# To use the shared wml-serving COS instance:\n",
    "!kubectl patch secret/storage-config -p '{\"data\": {\"wml-serving-example-models\": \"ewogICJ0eXBlIjogInMzIiwKICAiYWNjZXNzX2tleV9pZCI6ICJlY2I5ODNmMTE4MjI0MjNjYTllNDg3Zjg5OGQ1NGE4ZiIsCiAgInNlY3JldF9hY2Nlc3Nfa2V5IjogImNkYmVmZjZhMzJhZWY2YzIzNzRhZTY5ZWVmNTAzZTZkZDBjOTNkNmE3NGJjMjQ2NyIsCiAgImVuZHBvaW50X3VybCI6ICJodHRwczovL3MzLnVzLXNvdXRoLmNsb3VkLW9iamVjdC1zdG9yYWdlLmFwcGRvbWFpbi5jbG91ZCIsCiAgInJlZ2lvbiI6ICJ1cy1zb3V0aCIsCiAgImRlZmF1bHRfYnVja2V0IjogIndtbC1zZXJ2aW5nLWV4YW1wbGUtbW9kZWxzLXB1YmxpYyIKfQo=\"}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88775f53dfcc4d3f8d4cb8ada35c353d"
   },
   "source": [
    "### Submit Predictor CR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictor.wmlserving.ai.ibm.com/example-tensorflow-mnist created\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# SecretKey \"wml-serving-example-models\" added above is used\n",
    "cat <<EOF | kubectl apply -f -\n",
    "apiVersion: wmlserving.ai.ibm.com/v1\n",
    "kind: Predictor\n",
    "metadata:\n",
    "  name: example-tensorflow-mnist\n",
    "spec:\n",
    "  modelType:\n",
    "    name: tensorflow\n",
    "  path: tensorflow/mnist.savedmodel\n",
    "  storage:\n",
    "    s3:\n",
    "      secretKey: wml-serving-example-models\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictor.wmlserving.ai.ibm.com/example-sklearn-mnist-svm created\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# SecretKey \"wml-serving-example-models\" added above is used\n",
    "cat <<EOF | kubectl apply -f -\n",
    "apiVersion: wmlserving.ai.ibm.com/v1\n",
    "kind: Predictor\n",
    "metadata:\n",
    "  name: example-sklearn-mnist-svm\n",
    "spec:\n",
    "  modelType:\n",
    "    name: sklearn\n",
    "  path: sklearn/mnist-svm.joblib\n",
    "  storage:\n",
    "    s3:\n",
    "      secretKey: wml-serving-example-models\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Predictors Status\n",
    "\n",
    "This may take a few minutes to load while the runtime pods are brought up.\n",
    "Wait for the state of the Predictors to become 'Loaded'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                        TYPE         AVAILABLE   ACTIVEMODEL   TARGETMODEL   TRANSITION   AGE\n",
      "example-sklearn-mnist-svm   sklearn      true        Loaded                      UpToDate     2m7s\n",
      "example-tensorflow-mnist    tensorflow   true        Loaded                      UpToDate     2m34s\n",
      "NAME                                             READY   STATUS    RESTARTS   AGE\n",
      "wml-serving-msp-ml-server-0.x-57c69c6bfb-vl9wf   3/3     Running   1          2m7s\n",
      "wml-serving-msp-ml-server-0.x-57c69c6bfb-x7rbt   3/3     Running   1          2m7s\n",
      "wml-serving-triton-2.x-6c9784c9db-m875r          3/3     Running   0          2m34s\n",
      "wml-serving-triton-2.x-6c9784c9db-shq9f          3/3     Running   0          2m34s\n",
      "wmlserving-controller-568c45b959-xfzlk           1/1     Running   0          13m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get predictors\n",
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference using KFServing V2 dataplane API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dataset = np.load(\"mnist.npz\")\n",
    "x_test = dataset['x_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHEAAABxCAYAAADifkzQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAADsklEQVR4nO2cv0tyYRTHry9FQREhDREYREERBA0O2aCDESQ1RDT1Hzg0tjfXWEPon6BLiLqIRg6BQbQ0OFVLEFRDQxBJ7/DC4Tn3xbTyXu/9+v1M53AkD3w4z9Nzfxj4/Py0iL/50+0GyO+hRAAoEQBKBIASAaBEAPpa1Hn+8A6BZgVOIgCUCAAlAkCJAFAiAJQIACUCQIkAUCIAlAgAJQJAiQBQIgCUCAAlAkCJAFAiAJQIACUCQIkAUCIAlAgAJQJAiQC0eni442QyGZWnUimJJyYmVG1wcFDinZ0dVRsfH5d4Zmamky36Dk4iAJQIQKDF694dfxdjampK5be3tz/6OyMjIxLPz8//pqUfEQqFJN7b21O1cDjsxFfyXQxkKBEASgTA9SNGOp1W+fX1tcT2ve3m5kbiq6srVatUKhJfXFyo2uTkpMT39/dt99bf36/ysbExiR8eHlTN/E5zf7Qsx/bEpnASAaBEAFw/YnSKl5cXie1Lrbmc1Wq1tv/mwMCAymdnZyWem5tTtefnZ4mPj49VLZlMtv2d34BHDGQoEQBKBMC3e6IbZLNZibe3t1VtYWFB4nK5rGrBYNCJdrgnIkOJAHA5NXh8fFS5uWTaa+bN7a2tLWcb+weXU2QoEQBKBMD1uxhexn75zNwHR0dHVc28JNdtOIkAUCIAPX/EqFarEsfjcVV7f3+X+OzsTNWi0aizjf0PjxjIUCIAlAhAzx8x8vm8xOYeaFmWtbKyInEkEnGtp+/CSQSAEgGgRAB6bk98e3tTebFYlNj+tNv+/r7E9geLvQQnEQBKBKDnltODgwOVmw8er62tqdry8rIrPf0WTiIAlAgAJQIAfysql8upfHNzU+VDQ0MSFwoFVfPYpTbeikKGEgGAPGI8PT1JvLu7q2ofHx8qTyQSEnts+WwbTiIAlAgAJQIAccRoNBoqX1pakvjy8lLV7L/IaN7FmJ6edqC7jsEjBjKUCADEclqv11X+1XsSp6enKt/Y2HCkJwfgcooMJQJAiQD49rLb3d2dxKurq00/d3h4qPL19XXHeuoWnEQAKBEA3y6nJycnEptLq51YLKbyQKDpf+q+hZMIACUCQIkA+GZPPD8/V/nR0VGXOvEenEQAKBEA3yyn5k+VWJZlvb6+Nv2seeN3eHjYsZ68AicRAEoEgBIB8M2e+BWLi4sqL5VKEjv0o+qegpMIACUCAPGgVI/AB6WQoUQAKBGAVkcMvNvggHASAaBEACgRAEoEgBIBoEQA/gJOwcDT7A4oHgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Image should look like 7\n",
    "for i, image in enumerate([x_test[0]]):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_0 = x_test[0].flatten().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "Run the following command in a separate terminal to forward grpc calls to the cluster. This is needed for inferencing.\n",
    "\n",
    "`kubectl port-forward --address 0.0.0.0 service/wml-serving 8033 -n wmlserving`\n",
    "\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "aba44afdc3a74575959ce111901e4560"
   },
   "outputs": [],
   "source": [
    "channel = grpc.insecure_channel('0.0.0.0:8033')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_client = grpc_service_pb2_grpc.GRPCInferenceServiceStub(channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_contents = grpc_service_pb2.InferTensorContents(fp32_contents=score_0)\n",
    "infer_input=grpc_service_pb2.ModelInferRequest().InferInputTensor(name=\"inputs\",shape=[1,784],datatype=\"FP32\",contents=tensor_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name : example-tensorflow-mnist-cca986bcf1\n",
      "model_version : 1\n",
      "prediction : 7\n"
     ]
    }
   ],
   "source": [
    "# Expect prediction to be 7\n",
    "inputs=[]\n",
    "inputs.append(infer_input)\n",
    "request=grpc_service_pb2.ModelInferRequest(model_name=\"example-tensorflow-mnist\",inputs=inputs)\n",
    "\n",
    "results,call=infer_client.ModelInfer.with_call(request=request)\n",
    "print(\"model_name : \"+results.model_name+\"\\n\"+\"model_version : \"+results.model_version + \"\\n\"\n",
    "     \"prediction : \"+str(list(results.raw_output_contents[0])[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLServer Inference\n",
    "\n",
    "Using grpcurl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grpcurl 1.8.1\n"
     ]
    }
   ],
   "source": [
    "# !brew install grpcurl\n",
    "!grpcurl --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference.GRPCInferenceService.ModelInfer\n",
      "inference.GRPCInferenceService.ModelMetadata\n",
      "inference.GRPCInferenceService.ModelReady\n",
      "inference.GRPCInferenceService.ServerLive\n",
      "inference.GRPCInferenceService.ServerMetadata\n",
      "inference.GRPCInferenceService.ServerReady\n"
     ]
    }
   ],
   "source": [
    "!grpcurl -plaintext -proto fvt/proto/kfs_inference_v2.proto 0.0.0.0:8033 list inference.GRPCInferenceService"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Run the following command in a separate terminal to forward grpc calls to the cluster. This is needed for inferencing.\n",
    "\n",
    "`kubectl port-forward --address 0.0.0.0 service/wml-serving 8033 -n wmlserving`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"modelName\": \"example-sklearn-mnist-svm-725d74f061\",\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"name\": \"predict\",\n",
      "      \"datatype\": \"FP32\",\n",
      "      \"shape\": [\n",
      "        \"1\"\n",
      "      ],\n",
      "      \"contents\": {\n",
      "        \"fp32Contents\": [\n",
      "          8\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Expect contents of output to be 8\n",
    "!grpcurl -plaintext -proto fvt/proto/kfs_inference_v2.proto -d '{ \"model_name\": \"example-sklearn-mnist-svm\", \"inputs\": [{ \"name\": \"predict\", \"shape\": [1, 64], \"datatype\": \"FP32\", \"contents\": { \"fp32_contents\": [0.0, 0.0, 1.0, 11.0, 14.0, 15.0, 3.0, 0.0, 0.0, 1.0, 13.0, 16.0, 12.0, 16.0, 8.0, 0.0, 0.0, 8.0, 16.0, 4.0, 6.0, 16.0, 5.0, 0.0, 0.0, 5.0, 15.0, 11.0, 13.0, 14.0, 0.0, 0.0, 0.0, 0.0, 2.0, 12.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 13.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 16.0, 16.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 11.0, 13.0, 12.0, 1.0, 0.0] }}]}' 0.0.0.0:8033 inference.GRPCInferenceService.ModelInfer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
