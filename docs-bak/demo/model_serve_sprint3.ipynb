{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "281a75273d4748a1a6687d9d0d2b2c50"
   },
   "outputs": [],
   "source": [
    "# Project Model-Mesh Serving Sprint 3 Demo Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Setup (Run this before the demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9edb1c5cc97049d387474cfdce84a5d9"
   },
   "outputs": [],
   "source": [
    "%cd ../../config/install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45bee5c451714be6912b060f3c73af39"
   },
   "outputs": [],
   "source": [
    "# install python dependencies\n",
    "!pip install grpcio grpcio-tools numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data for MNIST\n",
    "!wget -nv https://s3.amazonaws.com/img-datasets/mnist.npz\n",
    "\n",
    "# Download triton grpc protos\n",
    "!wget -nv -O model_config.proto \"https://github.com/triton-inference-server/server/blob/master/src/core/model_config.proto?raw=true\"\n",
    "!wget -nv -O grpc_service.proto \"https://github.com/triton-inference-server/server/blob/master/src/core/grpc_service.proto?raw=true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "Run the following command in a separate terminal to forward grpc calls to the cluster. This is needed later during inferencing.\n",
    "\n",
    "`kubectl port-forward --address 0.0.0.0 service/model-mesh 8033 -n model-serving`\n",
    "\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. grpc_service.proto model_config.proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import grpc_service_pb2\n",
    "import grpc_service_pb2_grpc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4520043df94e40c38679956f18f1ad21"
   },
   "source": [
    "### Cluster Setup (Run this before the demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-requisites \n",
    "- ETCD for meta store\n",
    "- COS/S3 for model storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure you are logged into the cluster with kubectl or oc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "abfe4ba9ed8a4edd847bbc73f3b42565"
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ['KUBECONFIG']='/Users/tnarayan/AI/KUBE/stg-watson/kube-config-aaa00-stgwat-us-south-mzr-cruiser6.yml'\n",
    "# Tested on ocp cluster using oc login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Created namespace by name 'model-serving' set as default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create service account,roles with required authorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kustomize build rbac | kubectl apply -f -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Pull secret for public artifactory ( update docker user name and API key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl create secret docker-registry swg-devops-registry --docker-server=wcp-ai-foundation-team-docker-virtual.artifactory.swg-devops.com --docker-username=<artifactory user> --docker-password=<artifactory api key>\n",
    "\n",
    "# Add to both the wmlserving and wmlserving-controller service accounts\n",
    "!kubectl patch serviceaccount wmlserving -p '{\"imagePullSecrets\": [{\"name\": \"swg-devops-registry\"}]}'\n",
    "!kubectl patch serviceaccount wmlserving-controller -p '{\"imagePullSecrets\": [{\"name\": \"swg-devops-registry\"}]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ETCD Secret required prior to Controller installation, verify secret existence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl describe secret model-serving-etcd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bdc6580aa1f43029359eee926690ba3"
   },
   "source": [
    "#### CRD Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3afbf1a5f5b9451485095df766f41e53"
   },
   "outputs": [],
   "source": [
    "! kustomize build crd | kubectl apply -f -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23b5092fc1ed489c845a3ad4601f4f8d"
   },
   "source": [
    "#### Model Serve Controller Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7c57ecae76ed44b6ad5fcf6f2be52066"
   },
   "outputs": [],
   "source": [
    "! kustomize build controller | kubectl apply -f -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get deployment -l control-plane=wmlserving-controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-req for Triton serving for OCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!oc adm policy add-scc-to-user anyuid system:serviceaccount:model-serving:wmlserving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cb7a68822a3408cb5922dd9a5babfbe"
   },
   "source": [
    "# Sprint 3 Demo Summary\n",
    "\n",
    "- Model serve controller deployment\n",
    "- CRDs deployment\n",
    "- Runtime CR deployment ( can be TF and/or Triton )\n",
    "- Model deployment and Serving using KFServing V2 Protocol "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0501a66668941b68c5681f517781ba9"
   },
   "source": [
    "## ServingRuntime(s) CR installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get servingruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get deployment -l wmlserving-service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat runtimes/triton-2.30.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kustomize build runtimes | kubectl apply -f -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get deployment -l wmlserving-service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model management using Predictor CR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Secret Key for storage should be added to secret \"storage-config\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "    name: storage-config\n",
    "stringData:\n",
    "    myStorage: |\n",
    "        {\n",
    "            \"type\": \"s3\",\n",
    "            \"access_key_id\": \"xxx\",\n",
    "            \"secret_access_key\": \"xxx\",\n",
    "            \"endpoint_url\": \"https://s3.us-south.cloud-object-storage.appdomain.cloud\",\n",
    "            \"region\": \"us-south\",\n",
    "            \"default_bucket\": \"\"\n",
    "        }\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl patch secret storage-config -p '{\"data\":{\"myStorage\":\"<base64 encoded json>\"}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88775f53dfcc4d3f8d4cb8ada35c353d"
   },
   "source": [
    "### Submit Predictor CR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# SecretKey \"myStorage\" added above is used\n",
    "cat <<EOF | kubectl apply -f -\n",
    "apiVersion: ai.ibm.com/v1\n",
    "kind: Predictor\n",
    "metadata:\n",
    "  name: minimal-tf-predictor\n",
    "spec:\n",
    "  modelType:\n",
    "    name: tensorflow\n",
    "  path: tfmnist\n",
    "  storage:\n",
    "    s3:\n",
    "      secretKey: myStorage\n",
    "      bucket: triton-models\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# SecretKey \"myStorage\" added above is used\n",
    "cat <<EOF | kubectl apply -f -\n",
    "apiVersion: ai.ibm.com/v1\n",
    "kind: Predictor\n",
    "metadata:\n",
    "  name: minimal-mleap-predictor\n",
    "spec:\n",
    "  modelType:\n",
    "    name: mleap\n",
    "  path: example-model/airbnb.model.lr.zip\n",
    "  storage:\n",
    "    s3:\n",
    "      secretKey: myStorage\n",
    "      bucket: mleap-models\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Predictors Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference using KFServing V2 dataplane API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dataset = np.load(\"mnist.npz\")\n",
    "x_test = dataset['x_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, image in enumerate([x_test[0]]):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_0 = x_test[0].flatten().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aba44afdc3a74575959ce111901e4560"
   },
   "outputs": [],
   "source": [
    "channel = grpc.insecure_channel('localhost:8033')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_client = grpc_service_pb2_grpc.GRPCInferenceServiceStub(channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_contents = grpc_service_pb2.InferTensorContents(fp32_contents=score_0)\n",
    "infer_input=grpc_service_pb2.ModelInferRequest().InferInputTensor(name=\"inputs\",shape=[1,784],datatype=\"FP32\",contents=tensor_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata=(('mm-vmodel-id','minimal-tf-predictor'),)\n",
    "inputs=[]\n",
    "inputs.append(infer_input)\n",
    "request=grpc_service_pb2.ModelInferRequest(model_name=\"minimal-tf-predictor\",inputs=inputs)\n",
    "\n",
    "results,call=infer_client.ModelInfer.with_call(request=request,metadata=metadata)\n",
    "print(\"model_name : \"+results.model_name+\"\\n\"+\"model_version : \"+results.model_version + \"\\n\"\n",
    "     \"prediction : \"+str(list(results.raw_output_contents[0])[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch Predictor CR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### patch an existing Predictor with new version of same model from another path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat <<EOF | kubectl apply -f -\n",
    "apiVersion: ai.ibm.com/v1\n",
    "kind: Predictor\n",
    "metadata:\n",
    "  name: minimal-tf-predictor\n",
    "spec:\n",
    "  modelType:\n",
    "    name: tensorflow\n",
    "  path: tfmnistnew\n",
    "  storage:\n",
    "    s3:\n",
    "      secretKey: myStorage\n",
    "      bucket: triton-models\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on patched CR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata=(('mm-vmodel-id','minimal-tf-predictor'),)\n",
    "inputs=[]\n",
    "inputs.append(infer_input)\n",
    "results,call=infer_client.ModelInfer.with_call(request=request,metadata=metadata)\n",
    "print(\"model_name : \"+results.model_name+\"\\n\"+\"model_version : \"+results.model_version + \"\\n\"\n",
    "      \"prediction : \"+str(list(results.raw_output_contents[0])[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
